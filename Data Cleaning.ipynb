{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pywt\n",
    "from scipy import signal, stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import contextlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset-specific loaders\n",
    "def load_io_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load original IO format data (voluntary/involuntary)\"\"\"\n",
    "    df = pd.read_csv(filepath, sep=';', skiprows=2, usecols=range(11))\n",
    "    df.columns = ['SampleIndex', 'FP1', 'FP2', 'Channel3', 'Channel4', 'Channel5',\n",
    "                  'Channel6', 'Channel7', 'Channel8', 'Channel9', 'Channel10']\n",
    "    return df\n",
    "\n",
    "def load_vr_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load VR data with automatic format detection\"\"\"\n",
    "    try:\n",
    "        # Attempt to auto-detect format\n",
    "        df = pd.read_csv(\n",
    "            filepath,\n",
    "            sep=None,  # Auto-detect delimiter\n",
    "            engine='python',\n",
    "            skiprows=2,\n",
    "            on_bad_lines='warn',\n",
    "            dtype={'SampleIndex': int, 'FP1': float, 'FP2': float}\n",
    "        )\n",
    "        \n",
    "        # Column name normalization\n",
    "        df.columns = df.columns.str.strip().str.replace('[^A-Za-z0-9]+', '', regex=True)\n",
    "        \n",
    "        # Required columns check\n",
    "        required = {'SampleIndex', 'FP1', 'FP2', 'Blink'}\n",
    "        if required.issubset(df.columns):\n",
    "            return df[list(required)]\n",
    "            \n",
    "        # Fallback for column positions\n",
    "        return df.iloc[:, :4].set_axis(['SampleIndex', 'FP1', 'FP2', 'Blink'], axis=1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"VR load failed for {Path(filepath).name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def load_vv_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load VV data with proper column mapping\"\"\"\n",
    "    df = pd.read_csv(filepath, delimiter='\\t', skiprows=3)\n",
    "    return df.rename(columns={\n",
    "        'TimeStamp': 'Timestamp',\n",
    "        'Blink': 'BlinkFlag',\n",
    "        'Condition': 'TrialType'\n",
    "    })[['Timestamp', 'FP1', 'FP2', 'BlinkFlag', 'TrialType']]\n",
    "\n",
    "def load_labels(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load labels with validation\"\"\"\n",
    "    labels = pd.read_csv(filepath)\n",
    "    return labels[['StartSample', 'EndSample', 'Blink', 'TrialType']]\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Safer column standardization\"\"\"\n",
    "    # Convert timestamp first\n",
    "    if 'Timestamp' in df.columns:\n",
    "        df['SampleIndex'] = (df['Timestamp'] * 250).astype('int32')\n",
    "    \n",
    "    # Handle blink labels\n",
    "    blink_sources = ['Blink', 'BlinkFlag']\n",
    "    existing_blink = [col for col in blink_sources if col in df.columns]\n",
    "    df['Blink'] = df[existing_blink[0]] if existing_blink else 0\n",
    "    \n",
    "    # Map trial types\n",
    "    trial_map = {'voluntary': 1, 'V': 1, 'involuntary': 0, 'I': 0}\n",
    "    df['Voluntary'] = df['TrialType'].map(trial_map).fillna(0).astype('int8')\n",
    "    \n",
    "    return df[['SampleIndex', 'FP1', 'FP2', 'Blink', 'Voluntary', 'DatasetType']]\n",
    "\n",
    "# def _add_dataset_type(df: pd.DataFrame, dataset_type: str) -> pd.DataFrame:\n",
    "#     return df.assign(DatasetType=dataset_type)\n",
    "\n",
    "# def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"Ensure consistent column names and units\"\"\"\n",
    "#     # Convert timestamps to sample indices if needed\n",
    "#     if 'Timestamp' in df.columns:\n",
    "#         df['SampleIndex'] = (df['Timestamp'] * 250).astype(int)  # Assuming 250Hz\n",
    "    \n",
    "#     # Standardize blink labels\n",
    "#     df['Blink'] = df['Blink'].map({'yes': 1, 'no': 0}).fillna(0).astype(int)\n",
    "    \n",
    "#     # Convert voluntary to binary\n",
    "#     df['Voluntary'] = df['Voluntary'].map({'voluntary': 1, 'involuntary': 0}).fillna(0)\n",
    "    \n",
    "#     return df[['SampleIndex', 'FP1', 'FP2', 'Blink', 'Voluntary', 'DatasetType']]\n",
    "\n",
    "# # Denoise Method\n",
    "# def wavelet_denoise(signal_data, wavelet='db4', level=3):\n",
    "#     \"\"\"Improved wavelet denoising with auto-padding\"\"\"\n",
    "#     # Calculate required length (next multiple of 2^level)\n",
    "#     required_length = ((len(signal_data) + (2 ** level - 1)) // (2 ** level)) * (2 ** level)\n",
    "#     padded_signal = np.pad(signal_data, (0, required_length - len(signal_data)), \n",
    "#                          mode='edge')\n",
    "    \n",
    "#     # Now calculate max_level based on the padded signal length\n",
    "#     max_level = pywt.swt_max_level(len(padded_signal))\n",
    "#     adjusted_level = min(level, max_level)\n",
    "    \n",
    "#     if adjusted_level < 1:\n",
    "#         raise ValueError(f\"Cannot perform SWT with level {adjusted_level}. Need at least level 1.\")\n",
    "        \n",
    "#     # Perform SWT with adjusted level\n",
    "#     coeffs = pywt.swt(padded_signal, wavelet, level=adjusted_level)\n",
    "\n",
    "#     # Adaptive thresholding\n",
    "#     sigma = np.median(np.abs(coeffs[-1][1])) / 0.6745\n",
    "#     threshold = sigma * np.sqrt(2 * np.log(len(padded_signal)))\n",
    "    \n",
    "#     # Apply threshold to detail coefficients\n",
    "#     denoised_coeffs = [coeffs[0]]  # Keep approximation coefficients\n",
    "#     for c in coeffs[1:]:\n",
    "#         denoised_coeffs.append(pywt.threshold(c, threshold, mode='soft'))\n",
    "    \n",
    "#     # Reconstruct signal\n",
    "#     denoised = pywt.iswt(denoised_coeffs, wavelet)\n",
    "#     return denoised[:len(signal_data)]  # Remove padding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New unified processing pipeline\n",
    "def process_all_data(base_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Simplified processing pipeline\"\"\"\n",
    "    base_path = Path(base_path)\n",
    "    datasets = []\n",
    "    \n",
    "    # Unified dataset processor\n",
    "    def process_dataset(pattern, loader, needs_labels=False, dataset_type=None):\n",
    "        for file in base_path.glob(pattern):\n",
    "            try:\n",
    "                data = loader(file)\n",
    "                \n",
    "                # Handle labels\n",
    "                if needs_labels:\n",
    "                    label_file = file.parent / file.name.replace('_data.csv', '_labels.csv')\n",
    "                    labels = load_labels(label_file) if label_file.exists() else None\n",
    "                    data = pd.merge(data, labels, on='SampleIndex') if labels is not None else data\n",
    "                \n",
    "                data['DatasetType'] = dataset_type or file.parent.name.split('-')[-1]\n",
    "                datasets.append(data)\n",
    "                print(f\"✅ Processed {file.name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed {file.name}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    # Process all dataset types \n",
    "    process_dataset(\"EEG-IO/*_data.csv\", load_io_data, dataset_type='IO')\n",
    "    process_dataset(\"EEG-VR/*_data.csv\", load_vr_data, needs_labels=True)\n",
    "    process_dataset(\"EEG-VV/*_data.csv\", load_vv_data, needs_labels=True)\n",
    "    \n",
    "    # Add recalibration data\n",
    "    process_dataset(\"EEG-VR/*R_data.csv\", \n",
    "                   lambda f: pd.read_csv(f).rename(columns={'Time':'SampleIndex'})[['SampleIndex','FP1','FP2']],\n",
    "                   dataset_type='VR-Recal')\n",
    "\n",
    "    if not datasets:\n",
    "        raise ValueError(\"No valid data files found in any dataset folder\")\n",
    "    \n",
    "    return pd.concat(datasets).pipe(standardize_columns)\n",
    "\n",
    "# def process_all_data(base_path: str) -> pd.DataFrame:\n",
    "#     \"\"\"Main processing pipeline\"\"\"\n",
    "#     base_path = Path(base_path)\n",
    "#     dfs = []\n",
    "\n",
    "#     # Initialize all dataset lists\n",
    "#     io_dfs, vr_dfs, vv_dfs, recal_dfs = [], [], [], []\n",
    "\n",
    "#     # Process IO data\n",
    "#     try:\n",
    "#         io_files = list(base_path.glob(\"EEG-EyeBlinks/EEG-IO/*_data.csv\"))\n",
    "#         for io_file in io_files:\n",
    "#             try:\n",
    "#                 io_dfs.append(load_io_data(io_file))\n",
    "#                 print(f\"Processed IO file: {io_file.name}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Skipped IO file {io_file.name}: {str(e)}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"IO processing failed: {str(e)}\")\n",
    "\n",
    "#     # VR processing\n",
    "#     try:\n",
    "#         vr_files = list(base_path.glob(\"EEG-EyeBlinks/EEG-VR/*_data.csv\"))\n",
    "#         for vr_file in vr_files:\n",
    "#             try:\n",
    "#                 data = load_vr_data(f)\n",
    "#                 label_path = f.parent / f.name.replace('_data.csv', '_labels.csv')\n",
    "                \n",
    "#                 if label_path.exists():\n",
    "#                     labels = load_labels(label_path)\n",
    "#                     merged = pd.merge(data, labels, on='SampleIndex', how='left')\n",
    "#                 else:\n",
    "#                     print(f\"Warning: Missing labels for {f.name}, using defaults\")\n",
    "#                     merged = data.assign(Blink=0, TrialType='voluntary' if 'V' in f.stem else 'involuntary')\n",
    "                \n",
    "#                 vr_dfs.append(merged)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Skipped VR file {f.name}: {str(e)}\")\n",
    "#                 continue\n",
    "#     except Exception as e:\n",
    "#         print(f\"VR processing failed: {str(e)}\")\n",
    "    \n",
    "#     # Process VV data with labels\n",
    "#     vv_dfs = []\n",
    "#     for vv_file in base_path.glob(\"EEG-EyeBlinks/EEG-VV/*_data.csv\"):\n",
    "#         try:\n",
    "#             data = load_vv_data(vv_file)\n",
    "#             labels = load_labels(vv_file.parent / vv_file.name.replace('_data.csv', '_labels.csv'))\n",
    "#             merged = pd.merge(data, labels, left_on='Timestamp', right_on='StartSample')\n",
    "#             vv_dfs.append(merged)\n",
    "#             print(f\"Processed VV file: {vv_file.name}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Skipped VV file {vv_file.name}: {str(e)}\")\n",
    "\n",
    "\n",
    "#     # Process Recalibration files\n",
    "#     recal_dfs = []\n",
    "#     for recal_file in base_path.glob(\"EEG-VR/*R_data.csv\"):\n",
    "#         try:\n",
    "#             df = pd.read_csv(recal_file, sep='\\t', skiprows=3)\n",
    "#             df = df.rename(columns={\n",
    "#                 'Time': 'SampleIndex',\n",
    "#                 'EEG1': 'FP1',\n",
    "#                 'EEG2': 'FP2'\n",
    "#             })[['SampleIndex', 'FP1', 'FP2']]\n",
    "#             df['Blink'] = 0\n",
    "#             df['DatasetType'] = 'VR-Recal'\n",
    "#             dfs.append(df)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Skipped recalibration file {recal_file.name}: {str(e)}\")\n",
    "\n",
    "    \n",
    "#     # Combine all datasets\n",
    "#     return pd.concat([\n",
    "#         *(_add_dataset_type(df, 'IO') for df in io_dfs),\n",
    "#         *(_add_dataset_type(df, 'VR') for df in vr_dfs),\n",
    "#         *(_add_dataset_type(df, 'VV') for df in vv_dfs),\n",
    "#         *recal_dfs\n",
    "#     ]).pipe(standardize_columns)\n",
    "    \n",
    "    # # Standardize columns\n",
    "    # return combined_df.rename(columns={\n",
    "    #     'BlinkFlag': 'Blink',\n",
    "    #     'TrialType': 'Voluntary'\n",
    "    # }).pipe(standardize_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "def clean_eeg_data(df: pd.DataFrame, sfreq: float) -> pd.DataFrame:\n",
    "    \"\"\"Main cleaning pipeline\"\"\"\n",
    "    df = df.astype({\n",
    "        'SampleIndex': 'int32',\n",
    "        'Blink': 'int8',\n",
    "        'Voluntary': 'int8',\n",
    "        'DatasetType': 'category'\n",
    "    })\n",
    "    \n",
    "    # 1. Remove constant invalid values (-187500)\n",
    "    eeg_channels = ['FP1', 'FP2', 'Channel3', 'Channel4', 'Channel5']\n",
    "    df[eeg_channels] = df[eeg_channels].replace(-187500.02, np.nan)\n",
    "\n",
    "     # Preserve metadata columns through processing\n",
    "    metadata = df[['SampleIndex', 'Blink', 'Voluntary', 'DatasetType']]\n",
    "    processed = df.drop(columns=['Blink', 'Voluntary', 'DatasetType'])\n",
    "    # 2. Handle missing values\n",
    "    df = df.ffill().bfill()\n",
    "    \n",
    "    # 3. Remove non-EEG columns (using ACTUAL existing columns)\n",
    "    df = df[['SampleIndex'] + eeg_channels]  # Now matches loaded columns\n",
    "    \n",
    "    # 4. Convert to μV to Volts\n",
    "    df[eeg_channels] /= 1e6  # Convert from μV to V\n",
    "    \n",
    "    # 5. Outlier removal using Hampel filter\n",
    "    for ch in eeg_channels:\n",
    "        median = df[ch].rolling(window=100, center=True).median()\n",
    "        mad = np.abs(df[ch] - median).rolling(window=100, center=True).median()\n",
    "        df[ch] = np.where(np.abs(df[ch] - median) > 3*mad, median, df[ch])\n",
    "    \n",
    "    # 6. Bandpass filtering (1-40 Hz) with Nyquist check\n",
    "    nyquist = 0.5 * sfreq\n",
    "    low = 1.0\n",
    "    high = min(40.0, nyquist * 0.95)  # Ensure we stay below Nyquist\n",
    "    \n",
    "    # Validate frequency range\n",
    "    if low >= high:\n",
    "        raise ValueError(f\"Invalid filter range: low={low}Hz, high={high}Hz (Nyquist={nyquist}Hz)\")\n",
    "    \n",
    "    sos = signal.butter(2, [low, high], btype='bandpass', fs=sfreq, output='sos')\n",
    "    for ch in eeg_channels:\n",
    "        df[ch] = signal.sosfiltfilt(sos, df[ch])\n",
    "    \n",
    "    \n",
    "    # 7. Notch filter (50 Hz)\n",
    "    b, a = signal.iirnotch(50, 30, fs=sfreq)\n",
    "    for ch in eeg_channels:\n",
    "        df[ch] = signal.filtfilt(b, a, df[ch])\n",
    "    \n",
    "    # Merge back metadata\n",
    "    return pd.concat([metadata, processed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fatal error: No objects to concatenate\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Load with debug\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m         combined_raw \u001b[38;5;241m=\u001b[39m process_all_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEEG_EyeBlinks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# Add dtype check\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw dtypes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, combined_raw\u001b[38;5;241m.\u001b[39mdtypes)\n",
      "Cell \u001b[0;32mIn[83], line 75\u001b[0m, in \u001b[0;36mprocess_all_data\u001b[0;34m(base_path)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipped recalibration file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecal_file\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Combine all datasets\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat([\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;241m*\u001b[39m(_add_dataset_type(df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIO\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m io_dfs),\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;241m*\u001b[39m(_add_dataset_type(df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVR\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m vr_dfs),\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;241m*\u001b[39m(_add_dataset_type(df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVV\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m vv_dfs),\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;241m*\u001b[39mrecal_dfs\n\u001b[1;32m     80\u001b[0m ])\u001b[38;5;241m.\u001b[39mpipe(standardize_columns)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m    385\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[1;32m    386\u001b[0m     join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[1;32m    387\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[1;32m    388\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[1;32m    389\u001b[0m     names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[1;32m    390\u001b[0m     verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[1;32m    391\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clean_keys_and_objs(objs, keys)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load with debug\n",
    "    try:\n",
    "        combined_raw = process_all_data(\"EEG_EyeBlinks\")\n",
    "        # Add dtype check\n",
    "        print(\"Raw dtypes:\", combined_raw.dtypes)\n",
    "        \n",
    "        # Clean data\n",
    "        clean_df = clean_eeg_data(combined_raw, 250)\n",
    "        \n",
    "        # Final validation\n",
    "        assert not clean_df.isna().any().any(), \"NaNs in cleaned data\"\n",
    "        print(\"Processing complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dfs = []\n",
    "# sfreqs = set()\n",
    "\n",
    "# # Process all CSV files in EEG* subdirectories\n",
    "# print(\"Searching in:\", Path(\"EEG_EyeBlinks\").absolute())\n",
    "# found_files = list(Path(\"EEG_EyeBlinks\").glob(\"EEG*/**/*.csv\"))\n",
    "# print(f\"Found {len(found_files)} CSV files\")\n",
    "\n",
    "# for csv_file in found_files:\n",
    "#     # Add these checks before processing\n",
    "#     if not csv_file.name.lower().endswith(\"_data.csv\"):\n",
    "#         print(f\"Skipping non-data file: {csv_file.name}\")\n",
    "#         continue\n",
    "        \n",
    "#     try:\n",
    "#         df, sf = load_eeg_data(str(csv_file))\n",
    "#         print(f\"Successfully loaded {len(df)} samples from {csv_file.name}\")\n",
    "#         all_dfs.append(df)\n",
    "#         sfreqs.add(sf)\n",
    "#     except Exception as e:\n",
    "#         print(f\"SKIPPED {csv_file.name} - {str(e)}\")\n",
    "#         continue\n",
    "\n",
    "# # Verify consistent sample rates\n",
    "# if len(sfreqs) > 1:\n",
    "#     raise ValueError(f\"Multiple sample rates detected: {sfreqs}. All files must have the same rate.\")\n",
    "# if not sfreqs:\n",
    "#     raise ValueError(\"No EEG files found in EEG* subfolders\")\n",
    "\n",
    "# raw_df = pd.concat(all_dfs, ignore_index=True)\n",
    "# sfreq = sfreqs.pop()\n",
    "\n",
    "# print(f\"Loaded {len(raw_df):,} samples from {len(all_dfs)} files\")\n",
    "# print(\"Sample rate:\", sfreq)\n",
    "\n",
    "# # Clean data\n",
    "# clean_df = clean_eeg_data(raw_df, sfreq)\n",
    "\n",
    "# # Apply wavelet denoising to kept channel\n",
    "# eeg_channels = ['FP1', 'FP2', 'Channel3', 'Channel4', 'Channel5']\n",
    "\n",
    "# # Apply wavelet denoising\n",
    "# for ch in eeg_channels:\n",
    "#     clean_df[ch] = wavelet_denoise(clean_df[ch].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
