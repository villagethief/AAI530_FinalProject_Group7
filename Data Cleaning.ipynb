{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pywt\n",
    "from scipy import signal, stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import contextlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset-specific loaders\n",
    "def load_io_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load original IO format data (voluntary/involuntary)\"\"\"\n",
    "    df = pd.read_csv(filepath, sep=';', skiprows=2, usecols=range(11))\n",
    "    df.columns = ['SampleIndex', 'FP1', 'FP2', 'Channel3', 'Channel4', 'Channel5',\n",
    "                  'Channel6', 'Channel7', 'Channel8', 'Channel9', 'Channel10']\n",
    "    return df\n",
    "\n",
    "def load_vr_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load VR data with automatic format detection\"\"\"\n",
    "    try:\n",
    "        # Attempt to auto-detect format\n",
    "        df = pd.read_csv(\n",
    "            filepath,\n",
    "            sep=None,  # Auto-detect delimiter\n",
    "            engine='python',\n",
    "            skiprows=2,\n",
    "            on_bad_lines='warn',\n",
    "            dtype={'SampleIndex': int, 'FP1': float, 'FP2': float}\n",
    "        )\n",
    "        \n",
    "        # Column name normalization\n",
    "        df.columns = df.columns.str.strip().str.replace('[^A-Za-z0-9]+', '', regex=True)\n",
    "        \n",
    "        # Required columns check\n",
    "        required = {'SampleIndex', 'FP1', 'FP2', 'Blink'}\n",
    "        if required.issubset(df.columns):\n",
    "            return df[list(required)]\n",
    "            \n",
    "        # Fallback for column positions\n",
    "        return df.iloc[:, :4].set_axis(['SampleIndex', 'FP1', 'FP2', 'Blink'], axis=1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"VR load failed for {Path(filepath).name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def load_vv_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load VV data with proper column mapping\"\"\"\n",
    "    df = pd.read_csv(filepath, delimiter='\\t', skiprows=3)\n",
    "    return df.rename(columns={\n",
    "        'TimeStamp': 'Timestamp',\n",
    "        'Blink': 'BlinkFlag',\n",
    "        'Condition': 'TrialType'\n",
    "    })[['Timestamp', 'FP1', 'FP2', 'BlinkFlag', 'TrialType']]\n",
    "\n",
    "def load_labels(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load labels with validation\"\"\"\n",
    "    labels = pd.read_csv(filepath)\n",
    "    return labels[['StartSample', 'EndSample', 'Blink', 'TrialType']]\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Safer column standardization\"\"\"\n",
    "    # Convert timestamp first\n",
    "    if 'Timestamp' in df.columns:\n",
    "        df['SampleIndex'] = (df['Timestamp'] * 250).astype('int32')\n",
    "    \n",
    "    # Handle blink labels\n",
    "    blink_sources = ['Blink', 'BlinkFlag']\n",
    "    existing_blink = [col for col in blink_sources if col in df.columns]\n",
    "    df['Blink'] = df[existing_blink[0]] if existing_blink else 0\n",
    "    \n",
    "    # Map trial types\n",
    "    trial_map = {'voluntary': 1, 'V': 1, 'involuntary': 0, 'I': 0}\n",
    "    df['Voluntary'] = df['TrialType'].map(trial_map).fillna(0).astype('int8')\n",
    "    \n",
    "    return df[['SampleIndex', 'FP1', 'FP2', 'Blink', 'Voluntary', 'DatasetType']]\n",
    "\n",
    "# def _add_dataset_type(df: pd.DataFrame, dataset_type: str) -> pd.DataFrame:\n",
    "#     return df.assign(DatasetType=dataset_type)\n",
    "\n",
    "# def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"Ensure consistent column names and units\"\"\"\n",
    "#     # Convert timestamps to sample indices if needed\n",
    "#     if 'Timestamp' in df.columns:\n",
    "#         df['SampleIndex'] = (df['Timestamp'] * 250).astype(int)  # Assuming 250Hz\n",
    "    \n",
    "#     # Standardize blink labels\n",
    "#     df['Blink'] = df['Blink'].map({'yes': 1, 'no': 0}).fillna(0).astype(int)\n",
    "    \n",
    "#     # Convert voluntary to binary\n",
    "#     df['Voluntary'] = df['Voluntary'].map({'voluntary': 1, 'involuntary': 0}).fillna(0)\n",
    "    \n",
    "#     return df[['SampleIndex', 'FP1', 'FP2', 'Blink', 'Voluntary', 'DatasetType']]\n",
    "\n",
    "# # Denoise Method\n",
    "# def wavelet_denoise(signal_data, wavelet='db4', level=3):\n",
    "#     \"\"\"Improved wavelet denoising with auto-padding\"\"\"\n",
    "#     # Calculate required length (next multiple of 2^level)\n",
    "#     required_length = ((len(signal_data) + (2 ** level - 1)) // (2 ** level)) * (2 ** level)\n",
    "#     padded_signal = np.pad(signal_data, (0, required_length - len(signal_data)), \n",
    "#                          mode='edge')\n",
    "    \n",
    "#     # Now calculate max_level based on the padded signal length\n",
    "#     max_level = pywt.swt_max_level(len(padded_signal))\n",
    "#     adjusted_level = min(level, max_level)\n",
    "    \n",
    "#     if adjusted_level < 1:\n",
    "#         raise ValueError(f\"Cannot perform SWT with level {adjusted_level}. Need at least level 1.\")\n",
    "        \n",
    "#     # Perform SWT with adjusted level\n",
    "#     coeffs = pywt.swt(padded_signal, wavelet, level=adjusted_level)\n",
    "\n",
    "#     # Adaptive thresholding\n",
    "#     sigma = np.median(np.abs(coeffs[-1][1])) / 0.6745\n",
    "#     threshold = sigma * np.sqrt(2 * np.log(len(padded_signal)))\n",
    "    \n",
    "#     # Apply threshold to detail coefficients\n",
    "#     denoised_coeffs = [coeffs[0]]  # Keep approximation coefficients\n",
    "#     for c in coeffs[1:]:\n",
    "#         denoised_coeffs.append(pywt.threshold(c, threshold, mode='soft'))\n",
    "    \n",
    "#     # Reconstruct signal\n",
    "#     denoised = pywt.iswt(denoised_coeffs, wavelet)\n",
    "#     return denoised[:len(signal_data)]  # Remove padding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New unified processing pipeline\n",
    "def process_all_data(base_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Main processing pipeline\"\"\"\n",
    "    base_path = Path(base_path)\n",
    "    dfs = []\n",
    "    # Process IO data\n",
    "    for io_file in base_path.glob(\"EEG-IO/*_data.csv\"):\n",
    "        io_files = Path(base_path).glob(\"EEG-IO/*_data.csv\")\n",
    "        io_dfs = [load_io_data(f) for f in io_files]\n",
    "        \n",
    "    # VR processing\n",
    "    for vr_file in base_path.glob(\"EEG-VR/*_data.csv\"):\n",
    "        vr_files = Path(base_path).glob(\"EEG-VR/*_data.csv\")\n",
    "    vr_dfs = []\n",
    "    for f in vr_files:\n",
    "        data = load_vr_data(f)\n",
    "        labels = load_labels(f.parent / f.name.replace('_data.csv', '_labels.csv'))\n",
    "        vr_dfs.append(pd.merge(data, labels, on='SampleIndex'))\n",
    "        # Add this error fallback:\n",
    "        if 'SampleIndex' not in merged.columns:\n",
    "            merged['SampleIndex'] = merged.index\n",
    "    \n",
    "    \n",
    "    # Process VV data with labels\n",
    "    for vv_file in base_path.glob(\"EEG-VV/*_data.csv\"):\n",
    "        vv_files = Path(base_path).glob(\"EEG-VV/*_data.csv\")\n",
    "        vv_dfs = []\n",
    "        for f in vv_files:\n",
    "            data = load_vv_data(f)\n",
    "            labels = load_labels(f.parent / f.name.replace('_data.csv', '_labels.csv'))\n",
    "            vv_dfs.append(pd.merge(data, labels, left_on='Timestamp', right_on='StartSample'))\n",
    "            if 'TrialType' not in merged.columns:\n",
    "                merged['TrialType'] = 'voluntary' if 'V' in vv_file.stem else 'involuntary'\n",
    "\n",
    "    \n",
    "    # Combine all datasets\n",
    "    combined_df = pd.concat([\n",
    "        _add_dataset_type(df, 'IO') for df in io_dfs] +\n",
    "        [_add_dataset_type(df, 'VR') for df in vr_dfs] +\n",
    "        [_add_dataset_type(df, 'VV') for df in vv_dfs]\n",
    "    )\n",
    "    \n",
    "    # Standardize columns\n",
    "    return combined_df.rename(columns={\n",
    "        'BlinkFlag': 'Blink',\n",
    "        'TrialType': 'Voluntary'\n",
    "    }).pipe(standardize_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "def clean_eeg_data(df: pd.DataFrame, sfreq: float) -> pd.DataFrame:\n",
    "    \"\"\"Main cleaning pipeline\"\"\"\n",
    "    df = df.astype({\n",
    "        'SampleIndex': 'int32',\n",
    "        'Blink': 'int8',\n",
    "        'Voluntary': 'int8',\n",
    "        'DatasetType': 'category'\n",
    "    })\n",
    "\n",
    "    # 1. Remove constant invalid values (-187500)\n",
    "    eeg_channels = ['FP1', 'FP2', 'Channel3', 'Channel4', 'Channel5']\n",
    "    df[eeg_channels] = df[eeg_channels].replace(-187500.02, np.nan)\n",
    "\n",
    "     # Preserve metadata columns through processing\n",
    "    metadata = df[['SampleIndex', 'Blink', 'Voluntary', 'DatasetType']]\n",
    "    processed = df.drop(columns=['Blink', 'Voluntary', 'DatasetType'])\n",
    "    # 2. Handle missing values\n",
    "    df = df.ffill().bfill()\n",
    "    \n",
    "    # 3. Remove non-EEG columns (using ACTUAL existing columns)\n",
    "    df = df[['SampleIndex'] + eeg_channels]  # Now matches loaded columns\n",
    "    \n",
    "    # 4. Convert to μV to Volts\n",
    "    df[eeg_channels] /= 1e6  # Convert from μV to V\n",
    "    \n",
    "    # 5. Outlier removal using Hampel filter\n",
    "    for ch in eeg_channels:\n",
    "        median = df[ch].rolling(window=100, center=True).median()\n",
    "        mad = np.abs(df[ch] - median).rolling(window=100, center=True).median()\n",
    "        df[ch] = np.where(np.abs(df[ch] - median) > 3*mad, median, df[ch])\n",
    "    \n",
    "    # 6. Bandpass filtering (1-40 Hz) with Nyquist check\n",
    "    nyquist = 0.5 * sfreq\n",
    "    low = 1.0\n",
    "    high = min(40.0, nyquist * 0.95)  # Ensure we stay below Nyquist\n",
    "    \n",
    "    # Validate frequency range\n",
    "    if low >= high:\n",
    "        raise ValueError(f\"Invalid filter range: low={low}Hz, high={high}Hz (Nyquist={nyquist}Hz)\")\n",
    "    \n",
    "    sos = signal.butter(2, [low, high], btype='bandpass', fs=sfreq, output='sos')\n",
    "    for ch in eeg_channels:\n",
    "        df[ch] = signal.sosfiltfilt(sos, df[ch])\n",
    "    \n",
    "    \n",
    "    # 7. Notch filter (50 Hz)\n",
    "    b, a = signal.iirnotch(50, 30, fs=sfreq)\n",
    "    for ch in eeg_channels:\n",
    "        df[ch] = signal.filtfilt(b, a, df[ch])\n",
    "    \n",
    "    # Merge back metadata\n",
    "    return pd.concat([metadata, processed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fatal error: cannot access local variable 'vr_files' where it is not associated with a value\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'vr_files' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Load with debug\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m         combined_raw \u001b[38;5;241m=\u001b[39m process_all_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEEG_EyeBlinks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# Add dtype check\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw dtypes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, combined_raw\u001b[38;5;241m.\u001b[39mdtypes)\n",
      "Cell \u001b[0;32mIn[61], line 15\u001b[0m, in \u001b[0;36mprocess_all_data\u001b[0;34m(base_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m     vr_files \u001b[38;5;241m=\u001b[39m Path(base_path)\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEEG-VR/*_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m vr_dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m vr_files:\n\u001b[1;32m     16\u001b[0m     data \u001b[38;5;241m=\u001b[39m load_vr_data(f)\n\u001b[1;32m     17\u001b[0m     labels \u001b[38;5;241m=\u001b[39m load_labels(f\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m f\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_labels.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'vr_files' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load with debug\n",
    "    try:\n",
    "        combined_raw = process_all_data(\"EEG_EyeBlinks\")\n",
    "        # Add dtype check\n",
    "        print(\"Raw dtypes:\", combined_raw.dtypes)\n",
    "        \n",
    "        # Clean data\n",
    "        clean_df = clean_eeg_data(combined_raw, 250)\n",
    "        \n",
    "        # Final validation\n",
    "        assert not clean_df.isna().any().any(), \"NaNs in cleaned data\"\n",
    "        print(\"Processing complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dfs = []\n",
    "# sfreqs = set()\n",
    "\n",
    "# # Process all CSV files in EEG* subdirectories\n",
    "# print(\"Searching in:\", Path(\"EEG_EyeBlinks\").absolute())\n",
    "# found_files = list(Path(\"EEG_EyeBlinks\").glob(\"EEG*/**/*.csv\"))\n",
    "# print(f\"Found {len(found_files)} CSV files\")\n",
    "\n",
    "# for csv_file in found_files:\n",
    "#     # Add these checks before processing\n",
    "#     if not csv_file.name.lower().endswith(\"_data.csv\"):\n",
    "#         print(f\"Skipping non-data file: {csv_file.name}\")\n",
    "#         continue\n",
    "        \n",
    "#     try:\n",
    "#         df, sf = load_eeg_data(str(csv_file))\n",
    "#         print(f\"Successfully loaded {len(df)} samples from {csv_file.name}\")\n",
    "#         all_dfs.append(df)\n",
    "#         sfreqs.add(sf)\n",
    "#     except Exception as e:\n",
    "#         print(f\"SKIPPED {csv_file.name} - {str(e)}\")\n",
    "#         continue\n",
    "\n",
    "# # Verify consistent sample rates\n",
    "# if len(sfreqs) > 1:\n",
    "#     raise ValueError(f\"Multiple sample rates detected: {sfreqs}. All files must have the same rate.\")\n",
    "# if not sfreqs:\n",
    "#     raise ValueError(\"No EEG files found in EEG* subfolders\")\n",
    "\n",
    "# raw_df = pd.concat(all_dfs, ignore_index=True)\n",
    "# sfreq = sfreqs.pop()\n",
    "\n",
    "# print(f\"Loaded {len(raw_df):,} samples from {len(all_dfs)} files\")\n",
    "# print(\"Sample rate:\", sfreq)\n",
    "\n",
    "# # Clean data\n",
    "# clean_df = clean_eeg_data(raw_df, sfreq)\n",
    "\n",
    "# # Apply wavelet denoising to kept channel\n",
    "# eeg_channels = ['FP1', 'FP2', 'Channel3', 'Channel4', 'Channel5']\n",
    "\n",
    "# # Apply wavelet denoising\n",
    "# for ch in eeg_channels:\n",
    "#     clean_df[ch] = wavelet_denoise(clean_df[ch].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
